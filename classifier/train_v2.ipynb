{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying letters\n",
    "\n",
    "This notebook contains a training script for a classifier that can recognize a few letters from a bitmap.\n",
    "Version 2: uses data from the wand to validate (\"transfer learning\", if you will)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pathlib\n",
    "import shutil\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dfbf34",
   "metadata": {},
   "source": [
    "create new dataset from wand drawn images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11540b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy all images of same class to single dir\n",
    "\n",
    "def moveWandFiles(source_folder:pathlib.Path, target_folder:pathlib.Path,letter):\n",
    "    target_folder= os.path.join(target_folder + letter )   \n",
    "    source_folder= os.path.join(source_folder + letter )   \n",
    "    Path(target_folder).mkdir(parents=True, exist_ok=True)\n",
    "    ii = 1\n",
    "    for image_file in Path(source_folder).rglob(\"*.bmp\"): # recursively find image paths\n",
    "        # Separate base from extension\n",
    "        base, extension = os.path.splitext(image_file.name)        \n",
    "        new_name = os.path.join(letter + \"_\" + str(ii) + extension)    \n",
    "        shutil.copy(image_file, Path(target_folder).joinpath(new_name))\n",
    "        ii += 1\n",
    "\n",
    "\n",
    "    \n",
    "letter = 'L'        \n",
    "OG_lettersInD = 'C:\\\\Users\\\\john\\\\Documents\\\\Arduino\\\\priWand\\\\letters\\\\'      \n",
    "destCollated = 'C:\\\\Users\\\\john\\\\Documents\\\\Arduino\\\\priWand\\\\letters\\\\allWandLetters\\\\'  \n",
    "\n",
    "rootimage = OG_lettersInD\n",
    "disdir = destCollated     \n",
    "\n",
    "moveWandFiles(rootimage, disdir,letter)\n",
    "        \n",
    "tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    destCollated,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=letter,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of letters we want to recognized\n",
    "_LETTERS = 'MOCL'\n",
    "# A set of letters used to train the \"other\" category. These shouldn't look\n",
    "# similar to any of the letters in _LETTERS\n",
    "_OTHER_LETTERS = 'FHIKRTY'\n",
    "\n",
    "\n",
    "# Labels in the emnist/letters dataset that should be used for training.\n",
    "_LABELS = [ord(c) - ord('A') + 1 for c in _LETTERS + _OTHER_LETTERS]\n",
    "\n",
    "_WANTED_LABELS = [ord(c) - ord('A') + 1 for c in _LETTERS]\n",
    "_OTHER_LABEL = 0\n",
    "_NUM_CLASSES = len(_LETTERS) + 1  # All \"other\" letters classified as 0.\n",
    "_BATCH_SIZE = 32\n",
    "\n",
    "# load train and test dataset\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (train_ds, test_ds), ds_info = tfds.load(\n",
    "        name='emnist/letters',\n",
    "        split=['train', 'test'],\n",
    "        shuffle_files=True,\n",
    "        with_info=True,\n",
    "        as_supervised=True,\n",
    "        decoders={\n",
    "            # Don't decode images, the dataset will get filtered,\n",
    "            # and we shouldn't decode what we don't use.\n",
    "            'image': tfds.decode.SkipDecoding(),\n",
    "        })\n",
    "\n",
    "    train_ds = prepare(train_ds, ds_info)\n",
    "    test_ds = prepare(test_ds, ds_info)\n",
    "\n",
    "    return (train_ds, test_ds), ds_info\n",
    "\n",
    "\n",
    "def prepare(dataset, ds_info):\n",
    "    labels = tf.constant(_LABELS, dtype=tf.int64)\n",
    "    wanted_labels = tf.constant(_WANTED_LABELS, dtype=tf.int64)\n",
    "\n",
    "    @tf.function\n",
    "    def is_wanted(image, label):\n",
    "        \"\"\"Returns true if label is in _LABELS\"\"\"\n",
    "        del image  # unused\n",
    "        return tf.math.reduce_any(label == labels)\n",
    "\n",
    "    @tf.function\n",
    "    def map_entry(image, label):\n",
    "        \"\"\"Transforms the image into the form our classifier will expect.\"\"\"\n",
    "        decoded = ds_info.features['image'].decode_example(image)\n",
    "        # Convert image to floats\n",
    "        image = tf.cast(decoded, tf.float32) / 255\n",
    "        # Images in emnist are transposed. Bring them back into normal direction.\n",
    "        image = tf.transpose(image, perm=[1, 0, 2])\n",
    "        label = tf.cond(\n",
    "            tf.math.reduce_any(label == wanted_labels),\n",
    "            # Relabel entries that are in _LETTERS to the range [1, len(_LETTERS)]\n",
    "            lambda: tf.argmax(tf.equal(wanted_labels, label)) + 1,\n",
    "            # Relabel entries in _OTHER_LETTERS to 0.\n",
    "            lambda: tf.constant(_OTHER_LABEL, dtype=tf.int64))\n",
    "        return image, label\n",
    "\n",
    "    return (dataset.filter(is_wanted).cache().shuffle(1000).map(\n",
    "        map_entry).batch(_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "\n",
    "def define_model():\n",
    "    # Train with images that randomly rotated, in any direction. This is\n",
    "    # because we can't tell the direction the wand is held.\n",
    "    # If we could, we might get better classification by reducing the range of\n",
    "    # random rotation.\n",
    "    factor=0.2\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.experimental.preprocessing.RandomRotation(\n",
    "            factor, interpolation='nearest'),\n",
    "    ])\n",
    "    model = Sequential()\n",
    "    model.add(data_augmentation)\n",
    "    model.add(\n",
    "        layers.Conv2D(16, (3, 3),\n",
    "                      activation='relu',\n",
    "                      kernel_initializer='he_uniform',\n",
    "                      input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(\n",
    "        layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      kernel_initializer='he_uniform'))\n",
    "    model.add(\n",
    "        layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      kernel_initializer='he_uniform'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(\n",
    "        layers.Dense(50, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dense(_NUM_CLASSES))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# trains a model\n",
    "\n",
    "\n",
    "def train_model(train_ds, test_ds):\n",
    "    \"\"\"Trains the model, and outputs evaluation stats to TensorBoard.\"\"\"\n",
    "    model = define_model()\n",
    "    logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,\n",
    "                                                     histogram_freq=1,\n",
    "                                                     profile_batch='500,520')\n",
    "    # fit model\n",
    "    history = model.fit(train_ds,\n",
    "                        epochs=10,\n",
    "                        validation_data=test_ds,\n",
    "                        callbacks=[tboard_callback],\n",
    "                        class_weight=_label_weights(len(_LABELS),\n",
    "                                                    _WANTED_LABELS))\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(test_ds)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    return model, acc, history\n",
    "\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "\n",
    "\n",
    "def summarize_diagnostics(histories):\n",
    "    for i in range(len(histories)):\n",
    "        # plot loss\n",
    "        pyplot.subplot(2, 1, 1)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(histories[i].history['val_loss'],\n",
    "                    color='orange',\n",
    "                    label='test')\n",
    "        # plot accuracy\n",
    "        pyplot.subplot(2, 1, 2)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(histories[i].history['accuracy'],\n",
    "                    color='blue',\n",
    "                    label='train')\n",
    "        pyplot.plot(histories[i].history['val_accuracy'],\n",
    "                    color='orange',\n",
    "                    label='test')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# summarize model performance\n",
    "\n",
    "\n",
    "def summarize_performance(scores):\n",
    "    # print summary\n",
    "    print('Accuracy: mean=%.3f std=%.3f, n=%d' %\n",
    "          (np.mean(scores) * 100, np.std(scores) * 100, len(scores)))\n",
    "    # box and whisker plots of results\n",
    "    pyplot.boxplot(scores)\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "\n",
    "\n",
    "def run_training():\n",
    "    \"\"\"Runs training and shows learning curves.\"\"\"\n",
    "    # load dataset\n",
    "    (train_ds, test_ds), ds_info = load_dataset()\n",
    "    # evaluate model\n",
    "    model, score, history = train_model(train_ds, test_ds)\n",
    "    # learning curves\n",
    "    summarize_diagnostics([history])\n",
    "    # summarize estimated performance\n",
    "    summarize_performance([score])\n",
    "    return model\n",
    "\n",
    "\n",
    "def _label_weights(total_num_labels, wanted_labels):\n",
    "    \"\"\"Reweight the label costs to account over represented OTHER label.\"\"\"\n",
    "    label_weights = {\n",
    "        l: 1.0 / (len(wanted_labels) + 1)\n",
    "        for l in range(len(wanted_labels) + 1)\n",
    "    }\n",
    "    label_weights[_OTHER_LABEL] = (1.0 /\n",
    "                                   ((len(wanted_labels) + 1) *\n",
    "                                    (total_num_labels - len(wanted_labels))))\n",
    "    return label_weights\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def blur(img_array):\n",
    "    kernel = np.array([1, 3, 1])\n",
    "    img_array = np.apply_along_axis(\n",
    "        lambda x: np.convolve(x, kernel, mode='same'), 0, img_array)\n",
    "    img_array = np.apply_along_axis(\n",
    "        lambda x: np.convolve(x, kernel, mode='same'), 1, img_array)\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    142/Unknown - 3s 14ms/step - loss: 0.0718 - accuracy: 0.5720"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7336/2229346447.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7336/4181615935.py\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;31m# learning curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0msummarize_diagnostics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7336/4181615935.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(train_ds, test_ds)\u001b[0m\n\u001b[0;32m    119\u001b[0m                                                      profile_batch='500,520')\n\u001b[0;32m    120\u001b[0m     \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     history = model.fit(train_ds,\n\u001b[0m\u001b[0;32m    122\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs\n",
    "\n",
    "# python -m tensorboard.main --logdir=logs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to tflite.\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "# open('model.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "modelSaveName = _LETTERS + '_'+ _OTHER_LETTERS+'.tflite'\n",
    "open(modelSaveName, 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the tflite model\n",
    "\n",
    "\n",
    "interpreter = tf.lite.Interpreter(modelSaveName)\n",
    "# interpreter = tf.lite.Interpreter('model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "input_tensor = interpreter.tensor(interpreter.get_input_details()[0][\"index\"])\n",
    "output_tensor = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "def classify(letter_image):\n",
    "    input_tensor()[:] = letter_image\n",
    "    interpreter.invoke()\n",
    "    probabilities = softmax(output_tensor()[0])\n",
    "    index = np.argmax(probabilities)\n",
    "    if not index:\n",
    "        return 'UNKNOWN', probabilities[index]\n",
    "    return _LETTERS[index - 1],  probabilities[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier on a few examples\n",
    "\n",
    "print('M letters')\n",
    "for i in range(3, 10):\n",
    "    image = Image.open(f\"evaluation/{i}_M.bmp\")\n",
    "    image = image.crop((0, 0, 28, 28))\n",
    "    np_image = np.array(image, np.float32)\n",
    "    np_image = np_image / 255.0\n",
    "    # Apply a blur to the input image to look more like the emnist training set.\n",
    "    np_image = blur(np_image)\n",
    "\n",
    "    pyplot.imshow(np_image, cmap='gray')\n",
    "    print(i, classify(np_image.reshape((1, 28, 28, 1))))\n",
    "    del image\n",
    "\n",
    "print('O letters')\n",
    "for i in range(3, 21):\n",
    "    image = Image.open(f\"evaluation/{i}_O.bmp\")\n",
    "    image = image.crop((0, 0, 28, 28))\n",
    "    np_image = np.array(image, np.float32)\n",
    "    np_image = np_image / 255.0\n",
    "    np_image = blur(np_image)\n",
    "    np_image = np_image / np.max(np_image)\n",
    "\n",
    "    pyplot.imshow(np_image, cmap='gray')\n",
    "    print(i, classify(np_image.reshape((1, 28, 28, 1))))\n",
    "    del image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
