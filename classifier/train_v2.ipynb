{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying letters\n",
    "\n",
    "This notebook contains a training script for a classifier that can recognize a few letters from a bitmap.\n",
    "Version 2: uses data from the wand to validate (\"transfer learning\", if you will)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pathlib\n",
    "import shutil\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dfbf34",
   "metadata": {},
   "source": [
    "create new dataset from wand drawn images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11540b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images collated\n"
     ]
    }
   ],
   "source": [
    "# first, copy all images of same class to single dir\n",
    "\n",
    "letter = 'M'       \n",
    "\n",
    "\n",
    "def moveWandFiles(source_folder:pathlib.Path, target_folder:pathlib.Path,letter):\n",
    "    target_folder= os.path.join(target_folder + letter )   \n",
    "    source_folder= os.path.join(source_folder + letter )   \n",
    "    Path(target_folder).mkdir(parents=True, exist_ok=True)\n",
    "    ii = 1\n",
    "    for image_file in Path(source_folder).rglob(\"*.bmp\"): # recursively find image paths\n",
    "        # Separate base from extension\n",
    "        base, extension = os.path.splitext(image_file.name)        \n",
    "        new_name = os.path.join(letter + \"_\" + str(ii) + extension)    \n",
    "        shutil.copy(image_file, Path(target_folder).joinpath(new_name))\n",
    "        ii += 1\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "OG_lettersInD = 'C:\\\\Users\\\\john\\\\Documents\\\\Arduino\\\\priWand\\\\letters\\\\'      \n",
    "destCollated = 'C:\\\\Users\\\\john\\\\Documents\\\\Arduino\\\\priWand\\\\priWand\\\\letters\\\\groupedWandLetters\\\\'  \n",
    "# destCollated = '..\\\\letters\\\\groupedWandLetters'  \n",
    "\n",
    "\n",
    "rootimage = OG_lettersInD\n",
    "disdir = destCollated     \n",
    "\n",
    "moveWandFiles(rootimage, disdir,letter)\n",
    "\n",
    "print(\"images collated\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "946deed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([12 13  3 15], shape=(4,), dtype=int64)\n",
      "Found 559 files belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_LETTERS = 'LMCO'\n",
    "\n",
    "# Labels in the emnist/letters dataset that should be used for training.\n",
    "_LABELS = [ord(c) - ord('A') + 1 for c in _LETTERS ]\n",
    "labels = tf.constant(_LABELS, dtype=tf.int64)\n",
    "print(labels)\n",
    "\n",
    "# _WANTED_LABELS = [ord(c) - ord('A') + 1 for c in _LETTERS]\n",
    "# _OTHER_LABEL = 0\n",
    "# _NUM_CLASSES = len(_LETTERS) + 1  # All \"other\" letters classified as 0.\n",
    "\n",
    "\n",
    "        \n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    destCollated,\n",
    "    labels=\"inferred\",\n",
    "    # label_mode=\"int\",\n",
    "    # class_names=labels,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(30, 30),\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")\n",
    "\n",
    "factor=0.1\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomRotation(\n",
    "        factor, interpolation='nearest',fill_mode = 'constant'),\n",
    "])\n",
    "\n",
    "# fill-mode = const: the input is extended by filling all values beyond the edge with the same constant value k = 0.\n",
    "\n",
    "# augmented_train_ds = val_ds.map(\n",
    "#     lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "\n",
    "\n",
    "# Apply `data_augmentation` to the val images.\n",
    "val_ds = val_ds.map(\n",
    "    lambda img, label: (data_augmentation(img), label),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "pyplot.figure(figsize=(10, 10))\n",
    "for images, _ in val_ds.take(1):\n",
    "    for i in range(9):\n",
    "        # augmented_images = data_augmentation(images)\n",
    "        ax = pyplot.subplot(3, 3, i + 1)\n",
    "        pyplot.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        pyplot.axis(\"off\")\n",
    "    \n",
    "pyplot.figure(figsize=(10, 10))\n",
    "for images, _ in val_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = pyplot.subplot(3, 3, i + 1)\n",
    "        pyplot.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        pyplot.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of letters we want to recognized\n",
    "_LETTERS = 'MOCL'\n",
    "# A set of letters used to train the \"other\" category. These shouldn't look\n",
    "# similar to any of the letters in _LETTERS\n",
    "_OTHER_LETTERS = 'FHIKRTY'\n",
    "\n",
    "\n",
    "# Labels in the emnist/letters dataset that should be used for training.\n",
    "_LABELS = [ord(c) - ord('A') + 1 for c in _LETTERS + _OTHER_LETTERS]\n",
    "\n",
    "_WANTED_LABELS = [ord(c) - ord('A') + 1 for c in _LETTERS]\n",
    "_OTHER_LABEL = 0\n",
    "_NUM_CLASSES = len(_LETTERS) + 1  # All \"other\" letters classified as 0.\n",
    "_BATCH_SIZE = 32\n",
    "\n",
    "# load train and test dataset\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (train_ds, test_ds), ds_info = tfds.load(\n",
    "        name='emnist/letters',\n",
    "        split=['train', 'test'],\n",
    "        shuffle_files=True,\n",
    "        with_info=True,\n",
    "        as_supervised=True,\n",
    "        decoders={\n",
    "            # Don't decode images, the dataset will get filtered,\n",
    "            # and we shouldn't decode what we don't use.\n",
    "            'image': tfds.decode.SkipDecoding(),\n",
    "        })\n",
    "\n",
    "    train_ds = prepare(train_ds, ds_info)\n",
    "    test_ds = prepare(test_ds, ds_info)\n",
    "\n",
    "    return (train_ds, test_ds), ds_info\n",
    "\n",
    "\n",
    "def prepare(dataset, ds_info):\n",
    "    labels = tf.constant(_LABELS, dtype=tf.int64)\n",
    "    wanted_labels = tf.constant(_WANTED_LABELS, dtype=tf.int64)\n",
    "\n",
    "    @tf.function\n",
    "    def is_wanted(image, label):\n",
    "        \"\"\"Returns true if label is in _LABELS\"\"\"\n",
    "        del image  # unused\n",
    "        return tf.math.reduce_any(label == labels)\n",
    "\n",
    "    @tf.function\n",
    "    def map_entry(image, label):\n",
    "        \"\"\"Transforms the image into the form our classifier will expect.\"\"\"\n",
    "        decoded = ds_info.features['image'].decode_example(image)\n",
    "        # Convert image to floats\n",
    "        image = tf.cast(decoded, tf.float32) / 255\n",
    "        # Images in emnist are transposed. Bring them back into normal direction.\n",
    "        image = tf.transpose(image, perm=[1, 0, 2])\n",
    "        label = tf.cond(\n",
    "            tf.math.reduce_any(label == wanted_labels),\n",
    "            # Relabel entries that are in _LETTERS to the range [1, len(_LETTERS)]\n",
    "            lambda: tf.argmax(tf.equal(wanted_labels, label)) + 1,\n",
    "            # Relabel entries in _OTHER_LETTERS to 0.\n",
    "            lambda: tf.constant(_OTHER_LABEL, dtype=tf.int64))\n",
    "        return image, label\n",
    "\n",
    "    return (dataset.filter(is_wanted).cache().shuffle(1000).map(\n",
    "        map_entry).batch(_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "\n",
    "def define_model():\n",
    "    # Train with images that randomly rotated, in any direction. This is\n",
    "    # because we can't tell the direction the wand is held.\n",
    "    # If we could, we might get better classification by reducing the range of\n",
    "    # random rotation.\n",
    "    factor=0.2\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.experimental.preprocessing.RandomRotation(\n",
    "            factor, interpolation='nearest'),\n",
    "    ])\n",
    "    model = Sequential()\n",
    "    model.add(data_augmentation)\n",
    "    model.add(\n",
    "        layers.Conv2D(16, (3, 3),\n",
    "                      activation='relu',\n",
    "                      kernel_initializer='he_uniform',\n",
    "                      input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(\n",
    "        layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      kernel_initializer='he_uniform'))\n",
    "    model.add(\n",
    "        layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      kernel_initializer='he_uniform'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(\n",
    "        layers.Dense(50, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dense(_NUM_CLASSES))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# trains a model\n",
    "\n",
    "\n",
    "def train_model(train_ds, test_ds):\n",
    "    \"\"\"Trains the model, and outputs evaluation stats to TensorBoard.\"\"\"\n",
    "    model = define_model()\n",
    "    logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,\n",
    "                                                     histogram_freq=1,\n",
    "                                                     profile_batch='500,520')\n",
    "    # fit model\n",
    "    history = model.fit(train_ds,\n",
    "                        epochs=10,\n",
    "                        validation_data=test_ds,\n",
    "                        callbacks=[tboard_callback],\n",
    "                        class_weight=_label_weights(len(_LABELS),\n",
    "                                                    _WANTED_LABELS))\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(test_ds)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    return model, acc, history\n",
    "\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "\n",
    "\n",
    "def summarize_diagnostics(histories):\n",
    "    for i in range(len(histories)):\n",
    "        # plot loss\n",
    "        pyplot.subplot(2, 1, 1)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(histories[i].history['val_loss'],\n",
    "                    color='orange',\n",
    "                    label='test')\n",
    "        # plot accuracy\n",
    "        pyplot.subplot(2, 1, 2)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(histories[i].history['accuracy'],\n",
    "                    color='blue',\n",
    "                    label='train')\n",
    "        pyplot.plot(histories[i].history['val_accuracy'],\n",
    "                    color='orange',\n",
    "                    label='test')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# summarize model performance\n",
    "\n",
    "\n",
    "def summarize_performance(scores):\n",
    "    # print summary\n",
    "    print('Accuracy: mean=%.3f std=%.3f, n=%d' %\n",
    "          (np.mean(scores) * 100, np.std(scores) * 100, len(scores)))\n",
    "    # box and whisker plots of results\n",
    "    pyplot.boxplot(scores)\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "\n",
    "\n",
    "def run_training():\n",
    "    \"\"\"Runs training and shows learning curves.\"\"\"\n",
    "    # load dataset\n",
    "    (train_ds, test_ds), ds_info = load_dataset()\n",
    "    # evaluate model\n",
    "    model, score, history = train_model(train_ds, test_ds)\n",
    "    # learning curves\n",
    "    summarize_diagnostics([history])\n",
    "    # summarize estimated performance\n",
    "    summarize_performance([score])\n",
    "    return model\n",
    "\n",
    "\n",
    "def _label_weights(total_num_labels, wanted_labels):\n",
    "    \"\"\"Reweight the label costs to account over represented OTHER label.\"\"\"\n",
    "    label_weights = {\n",
    "        l: 1.0 / (len(wanted_labels) + 1)\n",
    "        for l in range(len(wanted_labels) + 1)\n",
    "    }\n",
    "    label_weights[_OTHER_LABEL] = (1.0 /\n",
    "                                   ((len(wanted_labels) + 1) *\n",
    "                                    (total_num_labels - len(wanted_labels))))\n",
    "    return label_weights\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def blur(img_array):\n",
    "    kernel = np.array([1, 3, 1])\n",
    "    img_array = np.apply_along_axis(\n",
    "        lambda x: np.convolve(x, kernel, mode='same'), 0, img_array)\n",
    "    img_array = np.apply_along_axis(\n",
    "        lambda x: np.convolve(x, kernel, mode='same'), 1, img_array)\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    142/Unknown - 3s 14ms/step - loss: 0.0718 - accuracy: 0.5720"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7336/2229346447.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7336/4181615935.py\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;31m# learning curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0msummarize_diagnostics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7336/4181615935.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(train_ds, test_ds)\u001b[0m\n\u001b[0;32m    119\u001b[0m                                                      profile_batch='500,520')\n\u001b[0;32m    120\u001b[0m     \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     history = model.fit(train_ds,\n\u001b[0m\u001b[0;32m    122\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs\n",
    "\n",
    "# python -m tensorboard.main --logdir=logs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to tflite.\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "# open('model.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "modelSaveName = _LETTERS + '_'+ _OTHER_LETTERS+'.tflite'\n",
    "open(modelSaveName, 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the tflite model\n",
    "\n",
    "\n",
    "interpreter = tf.lite.Interpreter(modelSaveName)\n",
    "# interpreter = tf.lite.Interpreter('model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "input_tensor = interpreter.tensor(interpreter.get_input_details()[0][\"index\"])\n",
    "output_tensor = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "def classify(letter_image):\n",
    "    input_tensor()[:] = letter_image\n",
    "    interpreter.invoke()\n",
    "    probabilities = softmax(output_tensor()[0])\n",
    "    index = np.argmax(probabilities)\n",
    "    if not index:\n",
    "        return 'UNKNOWN', probabilities[index]\n",
    "    return _LETTERS[index - 1],  probabilities[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier on a few examples\n",
    "\n",
    "print('M letters')\n",
    "for i in range(3, 10):\n",
    "    image = Image.open(f\"evaluation/{i}_M.bmp\")\n",
    "    image = image.crop((0, 0, 28, 28))\n",
    "    np_image = np.array(image, np.float32)\n",
    "    np_image = np_image / 255.0\n",
    "    # Apply a blur to the input image to look more like the emnist training set.\n",
    "    np_image = blur(np_image)\n",
    "\n",
    "    pyplot.imshow(np_image, cmap='gray')\n",
    "    print(i, classify(np_image.reshape((1, 28, 28, 1))))\n",
    "    del image\n",
    "\n",
    "print('O letters')\n",
    "for i in range(3, 21):\n",
    "    image = Image.open(f\"evaluation/{i}_O.bmp\")\n",
    "    image = image.crop((0, 0, 28, 28))\n",
    "    np_image = np.array(image, np.float32)\n",
    "    np_image = np_image / 255.0\n",
    "    np_image = blur(np_image)\n",
    "    np_image = np_image / np.max(np_image)\n",
    "\n",
    "    pyplot.imshow(np_image, cmap='gray')\n",
    "    print(i, classify(np_image.reshape((1, 28, 28, 1))))\n",
    "    del image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
